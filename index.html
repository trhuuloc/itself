<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="ITSELF: Attention Guided Fine-Grained Alignment for Vision–Language Retrieval">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="Vision–language models (VLMs) have rapidly advanced and and show strong promise for text-based person search (TBPS), a task that requires capturing fine-grained relationships between images and text to distinguish individuals. Previous methods address these challenges through local alignment, yet they are often prone to shortcut learning and spurious correlations, yielding misalignment. Moreover, injecting prior knowledge can distort intra-modality structure. Motivated by our observation that encoder attention surfaces spatially precise evidence from the earliest training epochs \emph{and} to alleviate these issues, we introduce ITSELF, an attention-guided framework for \emph{implicit local alignment}. At its core, Guided Representation with Attentive Bank (GRAB) converts the model’s own attention into an Attentive Bank of high-saliency tokens and applies local objectives on this bank, learning fine-grained correspondences without extra supervision. To make the selection reliable and non-redundant, we introduce Multi-Layer Attention for Robust Selection (MARS), which aggregates attention across layers and performs diversity-aware top-k selection; and Adaptive Token Scheduler (ATS), which schedules the retention budget from coarse to fine over training, preserving context early while progressively focusing on discriminative details. Extensive experiments on three widely used TBPS benchmarks show \textbf{state-of-the-art} performance and strong cross-dataset generalization, confirming the effectiveness and robustness of our approach without additional prior supervision.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="Text-based Person Retrieval, Vision-Language Model">
  <!-- TODO: List all authors -->
  <meta name="author" content="Tien-Huy Nguyen, Huu-Loc Tran, Thanh Duc Ngo">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="University of Information Technology, VNU-HCM, Vietnam">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="ITSELF: Attention Guided Fine-Grained Alignment for Vision–Language Retrieval">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="Vision–language models (VLMs) have rapidly advanced and and show strong promise for text-based person search (TBPS), a task that requires capturing fine-grained relationships between images and text to distinguish individuals. Previous methods address these challenges through local alignment, yet they are often prone to shortcut learning and spurious correlations, yielding misalignment. Moreover, injecting prior knowledge can distort intra-modality structure. Motivated by our observation that encoder attention surfaces spatially precise evidence from the earliest training epochs \emph{and} to alleviate these issues, we introduce ITSELF, an attention-guided framework for \emph{implicit local alignment}. At its core, Guided Representation with Attentive Bank (GRAB) converts the model’s own attention into an Attentive Bank of high-saliency tokens and applies local objectives on this bank, learning fine-grained correspondences without extra supervision. To make the selection reliable and non-redundant, we introduce Multi-Layer Attention for Robust Selection (MARS), which aggregates attention across layers and performs diversity-aware top-k selection; and Adaptive Token Scheduler (ATS), which schedules the retention budget from coarse to fine over training, preserving context early while progressively focusing on discriminative details. Extensive experiments on three widely used TBPS benchmarks show \textbf{state-of-the-art} performance and strong cross-dataset generalization, confirming the effectiveness and robustness of our approach without additional prior supervision.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="PAPER_TITLE">
  <meta name="citation_author" content="FIRST_AUTHOR_LAST, FIRST_AUTHOR_FIRST">
  <meta name="citation_author" content="SECOND_AUTHOR_LAST, SECOND_AUTHOR_FIRST">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>ITSELF: Attention Guided Fine-Grained Alignment for Vision–Language Retrieval - AUTHOR_NAMES | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">ITSELF: Attention Guided Fine-Grained Alignment for Vision–Language Retrieval</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=-MRppsEAAAAJ&hl=vi" target="_blank">Tien-Huy Nguyen</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=1xlRrhMAAAAJ&hl=vi" target="_blank">Huu-Loc Tran</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com.vn/citations?user=I8bNZakAAAAJ&hl=vi" target="_blank">Thanh Duc Ngo</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block">University of Information Technology, VNU-HCM, Vietnam<br>WACV 2026</span>
                    <!-- TODO: Remove this line if no equal contribution -->
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2601.01024v1" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- TODO: Add your supplementary material PDF or remove this section -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a> -->
                  <!-- </span> -->

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <!-- <a href="https://github.com/AIVIETNAM-Hub/Hybrid-Unified-and-Iterative-A-Novel-Framework-for-Text-based-Person-Anomaly-Retrieval" target="_blank" -->
                    <!-- class="external-link button is-normal is-rounded is-dark"> -->
                    <!-- <span class="icon"> -->
                      <!-- <i class="fab fa-github"></i> -->
                    <!-- </span> -->
                    <span>Code - Coming Soon...</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/2504.08384" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
      <div class="column is-four-fifths">
      <h2 class="title"></h2>

      <img 
        src="static/images/Fig1test.png" 
        alt="First research result visualization" 
        loading="lazy"
        style="max-width: 50%; height: auto; display: block; margin-left: auto; margin-right: auto;"
      />

      <h2 class="subtitle center is-size-6 has-text-justified">
        <strong>Evolution of text-based person search paradigms.</strong> (a) Global matching method uses powerful MLLM to synthesize extra datasets (b) Recent local implicit matching method implicitly reasons over relations among all local tokens. (c) <strong>Ours - ITSELF with GRAB </strong>: an attention-guided local branch to learn implicitly fine-grained, discriminative features to achieve better alignment.
      </h2>
      </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
          Vision–language models (VLMs) have rapidly advanced and show strong promise for text-based person search (TBPS), a task that requires capturing fine-grained relationships between images and text to distinguish individuals. Previous methods address these challenges through local alignment, yet they are often prone to shortcut learning and spurious correlations, yielding misalignment. Moreover, injecting prior knowledge can distort intra-modality structure. Motivated by our observation that encoder attention surfaces spatially precise evidence from the earliest training epochs <em>and</em> to alleviate these issues, we introduce ITSELF, an attention-guided framework for <em>implicit local alignment</em>. At its core, Guided Representation with Attentive Bank (GRAB) converts the model’s own attention into an Attentive Bank of high-saliency tokens and applies local objectives on this bank, learning fine-grained correspondences without extra supervision. To make the selection reliable and non-redundant, we introduce Multi-Layer Attention for Robust Selection (MARS), which aggregates attention across layers and performs diversity-aware top-k selection; and Adaptive Token Scheduler (ATS), which schedules the retention budget from coarse to fine over training, preserving context early while progressively focusing on discriminative details. Extensive experiments on three widely used TBPS benchmarks show <strong>state-of-the-art</strong> performance and strong cross-dataset generalization, confirming the effectiveness and robustness of our approach without additional prior supervision.
        </p>

        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
      <div class="column is-four-fifths">
      <h2 class="title">Overview</h2>

      <img 
        src="static/images/pipeline.png" 
        alt="First research result visualization" 
        loading="lazy"
        style="max-width: 50%; height: auto; display: block; margin-left: auto; margin-right: auto;"
      />

      <h2 class="subtitle is-size-6 has-text-justified">
        <strong>Overview of our proposed ITSELF</strong> (an attention-guided implicit local alignment framework).
        The architecture features a dual-stream encoder for images (left) and text (right). At its core is the GRAB
        (Guided Representation with Attentive Bank) module, designed to learn fine-grained, discriminative cues. GRAB
        consists of MARS (Multi-layer Attention for Robust Selection), which fuses attention across layers to select
        informative patches/tokens, and ATS (Adaptive Token Scheduler), which anneals token selection from coarse to fine
        during training. The model is optimized with a dual-loss strategy: a local loss <i>L<sub>local</sub></i> aligns guided
        local representations, and a global loss <i>L<sub>global</sub></i> matches overall embeddings. ITSELF reinforces global
        text-image alignment without additional supervision or inference-time cost.
      </h2>
      </div>
      </div>
    </div>
  </div>
</section>




<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
      <div class="column is-four-fifths">
      <h2 class="title">Quantitative Results</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- TODO: Replace with your research result images -->
        <img src="static/images/CUHK.png" alt="First research result visualization" loading="lazy"/>
        <!-- TODO: Replace with description of this result -->
        <h2 class="subtitle has-text-centered">
          Performance of text-based person search methods on CUHK-PEDES.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/ICFG.png" alt="Second research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          Performance of text-based person search methods on ICFG-PEDES.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/RSTP.png" alt="Third research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
         Performance of text-based person search methods on RSTP-Reid.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/DomainGeneral.png" alt="Fourth research result visualization" loading="lazy" style="max-width: 50%; height: auto; display: block; margin-left: auto; margin-right: auto;"/>
      <h2 class="subtitle has-text-centered">
        Comparisons with state-of-the-arts(domain generalization). Here “C” denotes CUHK-PEDES, “I” represents ICFG-PEDES and "R" means RSTPReid.
      </h2>
    </div>
  </div>
  <p><strong>Comparison with SOTA methods.</strong> We compare our approach with recent methods on three benchmarks. Our model outperforms all CLIP-based competitors on every metric, setting new <strong>R@1</strong> and <strong>mAP</strong> records on all datasets, including <strong>RSTP-Reid</strong> (+2.17% mAP). Gains extend to <strong>R@5</strong> and <strong>R@10</strong>, showing broad retrieval improvements. Remarkably, we achieve this <em>without</em> ReID-domain pretraining, surpassing methods using larger backbones or extra resources. Notably, using only CLIP, we attain the top <strong>R@1</strong> on <strong>ICFG-PEDES</strong> over all methods.</p>

<p><strong>Domain Generalization.</strong> We test cross-domain performance by training on one dataset and evaluating on another. Our model consistently outperforms prior approaches across all six transfer settings (C→I, I→C, etc.), e.g., improving R@1 by over 2% in the challenging I→R setting. These results demonstrate robust text–image alignment and strong generalization across unseen domains.</p>

  </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
      <div class="column is-four-fifths">
      <h2 class="title">Qualitative Results</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- TODO: Replace with your research result images -->
        <img src="static/images/Quali_top10.png" alt="First research result visualization" loading="lazy"/>
        <!-- TODO: Replace with description of this result -->
        <h2 class="subtitle has-text-centered">
          Qualitative results of text-to-image retrieval on RSTPReid benchmark, comparing our method with RDE. Retrieved images are ranked from left to right in descending order of similarity. Correct matches are outlined in green, while incorrect ones are shown in red. Text highlighted in green indicates the descriptive details effectively captured by our approach.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/Quali_ICFG_gradcam.png" alt="Second research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          Qualitative comparison of attention maps generated by RDE and by ITSELF using the Grad-CAM.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/Quali_topK.png" alt="Third research result visualization" loading="lazy" style="max-width: 50%; height: auto; display: block; margin-left: auto; margin-right: auto;"/>
        <h2 class="subtitle has-text-centered">
         Comparison of selected top-K patches in text-to-image retrieval by the baseline and by ITSELF.
       </h2>
     </div>
  </div>
  <p><strong>Top-5 Retrieval Examples.</strong> Examples on the RSTPReid benchmark show our method outperforms RDE. For queries like a man in a black and blue jacket or a boy in a patterned black jacket, our approach retrieves more correct matches in the top-5, demonstrating better alignment of fine-grained textual descriptions with images.</p>

<p><strong>Attention Comparison.</strong> Grad-CAM visualizations reveal our model achieves sharper, query-focused attention on clothing and accessories, isolating target pedestrians in crowded scenes. RDE, by contrast, produces diffuse and sometimes irrelevant hotspots, highlighting our framework’s stronger attribute-level fidelity and reduced cross-identity confusion.</p>

<p><strong>Top-K Token Selection Analysis.</strong> Our top-K token selection generates focused, semantically relevant attention maps. It successfully highlights regions corresponding to keywords like "white coat," "red boots," and "gray backpack," improving text-to-image grounding compared to the scattered baseline attention.</p>

  </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
      <div class="column is-four-fifths">
      <h2 class="title">Ablation Study</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item" style="display: flex; flex-direction: column; align-items: center; justify-content: center; height: 400px; text-align: center;">
          <!-- Centered image -->
          <img src="static/images/ablation.png" 
              alt="Fourth research result visualization" 
              loading="lazy" 
              style="max-width: 100%; height: auto;"/>
          <h2 class="subtitle">
            Ablation study on each component of ITSELF on three datasets.
          </h2>
        </div>
        <div class="item">
        <!-- Your image here -->
        <img src="static/images/ablation_MARS_ATS.png" alt="Third research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
         <strong>(Left)</strong> Entropy of attention per CLIP layer across training epochs. <strong>(Middle)</strong> Ablation of which CLIP layers are selected for MARS (E: Early, M: Middle, L: Late). <strong>(Right)</strong> Sensitivity of the MARS discard ratio, evaluated on R@1 and mAP.
       </h2>
     </div>
  </div>
  <p><strong>Effectiveness of each component.</strong> We evaluate the contribution of each module in ITSELF across three datasets. The <strong>MARS module</strong> outperforms a fixed single-layer (SL) strategy, achieving R@1 gains of +2.24%, +3.01%, and +5.65%, showing the advantage of multi-layer attention. The <strong>ATS module</strong> further improves R@1, and combined with MARS, the full model achieves the strongest performance (+2.29%, +3.17%, +6.00%) by preserving discriminative cues and stabilizing optimization.</p>

<p><strong>Layer Selection (MARS) & Discard Ratio (ATS).</strong> Multi-layer configurations in MARS consistently outperform baselines, with the Middle+Late (M+L) combination giving the best R@1 and mAP. Early layer attention is sharply peaked with low semantic content, while middle layers capture broader context and late layers focus on

  </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->








<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      TODO: Replace with your poster PDF
      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
End paper poster -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@inproceedings{Nguyen_2026_WACV,
  title={ITSELF:AttentionGuidedFine-GrainedAlignmentforVision–Language Retrieval},
  author={Nguyen, Tien-Huy and Tran, Huu-Loc and Ngo, Thanh Duc},
  booktitle={Proceedings of the Winter Conference on Applications of Computer Vision (WACV)},
  pages={},
  year={2026}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
